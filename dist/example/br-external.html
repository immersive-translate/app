<!DOCTYPE html>
<html lang="en">
  <head>
    <title></title>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
  </head>
  <body>
    <p class="para speech">
      <span class="speaker">Dr Shah:</span
      ><span class="speechQA"
        >These things are going to learn what we feed them, and sometimes the
        patterns in the data are not the ones that we would like to believe are
        true. So I like to break it down into 2 parts. One is the creation of
        the model itself, and to the extent possible, we feed it content—imagine
        it as a diet for the model—that keeps it as unbiased as possible. It’s
        not perfect; there are things that are patterns of care that are just
        hardwired in. And then second is the policies that govern what happens
        when a model produces a certain output. We can be intentional about
        those policies, and for areas where we know that our care practices are
        not ideal, we say, “We will not trust the model output,” and we
        intentionally create the diet that we want to feed to these models.<br /><br />Take
        a simple example of producing a patient instruction after discharge or
        after an office visit. Historically it’s like 5 pages in English that a
        lot of people, including doctors, might have trouble following—“What
        exactly was I told?” Now if we train the model to produce exactly that,
        we help nobody. But we can be intentional and, say, “Produce a 1-page
        version. Produce it at the 10th-grade reading level. Produce it in the
        language of my choice,” and then we put it in an evaluation loop to say,
        “When that was produced, did it help? Did the patient follow the
        instruction better? Did they read it completely more often than not?”
        And then we use these feedback loops to steer us in directions that
        work.</span
      >
    </p>
  </body>
</html>
